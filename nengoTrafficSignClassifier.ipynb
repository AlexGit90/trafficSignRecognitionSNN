{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import nengo\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nengo_dl\n",
    "\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset \n",
    "root_dir = 'traffic-signs-preprocessed/'\n",
    "\n",
    "train = pickle.load(open(root_dir + 'train.pickle', 'rb'))\n",
    "test = pickle.load(open(root_dir + 'test.pickle', 'rb'))\n",
    "labels = pickle.load(open(root_dir + 'labels.pickle', 'rb'))\n",
    "\n",
    "NUM_CLASSES = 43\n",
    "\n",
    "train_images = train['features'][:, :, :, 0]\n",
    "train_labels = train['labels']\n",
    "train_images = train_images.reshape((train_images.shape[0], -1))\n",
    "\n",
    "test_images = test['features'][:, :, :, 0]\n",
    "test_labels = test['labels']\n",
    "test_images = test_images.reshape((test_images.shape[0], -1))\n",
    "\n",
    "print(train_images.shape)\n",
    "print(test_images.shape)\n",
    "\n",
    "# add single timestep to training data\n",
    "train_images = train_images[:, None, :]\n",
    "train_labels = train_labels[:, None, None]\n",
    "\n",
    "# when testing our network with spiking neurons we will need to run it\n",
    "# over time, so we repeat the input/target data for a number of\n",
    "# timesteps.\n",
    "n_steps = 25\n",
    "test_images = np.tile(test_images[:, None, :], (1, n_steps, 1))\n",
    "test_labels = np.tile(test_labels[:, None, None], (1, n_steps, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some examples\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    plt.imshow(np.reshape(train_images[i], (28, 28)), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(str(train_labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "# CNN with LIF neurons\n",
    "with nengo.Network(seed=0) as net:\n",
    "    # set some default parameters for the neurons that will make\n",
    "    # the training progress more smoothly\n",
    "    net.config[nengo.Ensemble].max_rates = nengo.dists.Choice([100])\n",
    "    net.config[nengo.Ensemble].intercepts = nengo.dists.Choice([0])\n",
    "    net.config[nengo.Connection].synapse = None\n",
    "    neuron_type = nengo.LIF(amplitude=0.01)\n",
    "\n",
    "    # this is an optimization to improve the training speed,\n",
    "    # since we won't require stateful behaviour in this example\n",
    "    nengo_dl.configure_settings(stateful=False)\n",
    "\n",
    "    # the input node that will be used to feed in input images\n",
    "    inp = nengo.Node(np.zeros(32 * 32))\n",
    "\n",
    "    # add the first convolutional layer\n",
    "    x = nengo_dl.Layer(tf.keras.layers.Conv2D(filters=32, kernel_size=3))(\n",
    "        inp, shape_in=(32, 32, 1)\n",
    "    )\n",
    "    x = nengo_dl.Layer(neuron_type)(x)\n",
    "\n",
    "    # add the second convolutional layer\n",
    "    x = nengo_dl.Layer(tf.keras.layers.Conv2D(filters=64, strides=2, kernel_size=3))(\n",
    "        x, shape_in=(30, 30, 32)\n",
    "    )\n",
    "    x = nengo_dl.Layer(neuron_type)(x)\n",
    "\n",
    "    # add the third convolutional layer\n",
    "    x = nengo_dl.Layer(tf.keras.layers.Conv2D(filters=128, strides=2, kernel_size=3))(\n",
    "        x, shape_in=(14, 14, 64)\n",
    "    )\n",
    "    x = nengo_dl.Layer(neuron_type)(x)\n",
    "\n",
    "    # linear readout\n",
    "    out = nengo_dl.Layer(tf.keras.layers.Dense(units=NUM_CLASSES))(x)\n",
    "\n",
    "    # we'll create two different output probes, one with a filter\n",
    "    # (for when we're simulating the network over time and\n",
    "    # accumulating spikes), and one without (for when we're\n",
    "    # training the network using a rate-based approximation)\n",
    "    out_p = nengo.Probe(out, label=\"out_p\")\n",
    "    out_p_filt = nengo.Probe(out, synapse=0.1, label=\"out_p_filt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the accuracy metric and minibatch size\n",
    "\n",
    "def classification_accuracy(y_true, y_pred):\n",
    "    return tf.metrics.sparse_categorical_accuracy(y_true[:, -1], y_pred[:, -1])\n",
    "\n",
    "minibatch_size = 128\n",
    "sim = nengo_dl.Simulator(net, minibatch_size=minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271/271 [==============================] - 142s 505ms/step - loss: 1.3840 - out_p_loss: 1.3840\n",
      "Epoch 2/15\n",
      "271/271 [==============================] - 145s 537ms/step - loss: 0.2490 - out_p_loss: 0.2490\n",
      "Epoch 3/15\n",
      "132/271 [=============>................] - ETA: 1:13 - loss: 0.1300 - out_p_loss: 0.1300"
     ]
    }
   ],
   "source": [
    "# run training or load pre-trained parameter\n",
    "do_training = False\n",
    "\n",
    "if do_training:\n",
    "    sim.compile(\n",
    "        optimizer=tf.optimizers.RMSprop(0.001),\n",
    "        loss={out_p: tf.losses.SparseCategoricalCrossentropy(from_logits=True)},\n",
    "    )\n",
    "    sim.fit(train_images, {out_p: train_labels}, epochs=15)\n",
    "    sim.save_params(\"./tafficSign_params\")\n",
    "else:\n",
    "    sim.load_params(\"./trafficSign_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "sim.compile(loss={out_p_filt: classification_accuracy})\n",
    "\n",
    "print(\n",
    "    \"Accuracy after training:\",\n",
    "    sim.evaluate(test_images, {out_p_filt: test_labels}, verbose=0)[\"loss\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions\n",
    "data = sim.predict(test_images[:minibatch_size])\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(test_images[i, 0].reshape((28, 28)), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(tf.nn.softmax(data[out_p_filt][i]))\n",
    "    plt.legend([str(i) for i in range(10)], loc=\"upper left\")\n",
    "    plt.xlabel(\"timesteps\")\n",
    "    plt.ylabel(\"probability\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08f6bde0ff86e9dd8e8ba03bbedd23a95eae2c6c02f83141ede41a8c4db964c9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
